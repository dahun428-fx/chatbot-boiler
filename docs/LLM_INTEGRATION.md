# LLM Integration Guide

ì´ ë¬¸ì„œëŠ” LLM Chatbot Boilerplateì—ì„œ LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ì§€ì› Provider](#-ì§€ì›-provider)
2. [ë¹ ë¥¸ ì‹œì‘](#-ë¹ ë¥¸-ì‹œì‘)
3. [í™˜ê²½ ì„¤ì •](#-í™˜ê²½-ì„¤ì •)
4. [API ì‚¬ìš©ë²•](#-api-ì‚¬ìš©ë²•)
5. [ìŠ¤íŠ¸ë¦¬ë°](#-ìŠ¤íŠ¸ë¦¬ë°)
6. [ì—ëŸ¬ ì²˜ë¦¬](#-ì—ëŸ¬-ì²˜ë¦¬)
7. [ì»¤ìŠ¤í…€ Provider ì¶”ê°€](#-ì»¤ìŠ¤í…€-provider-ì¶”ê°€)

---

## ğŸ”Œ ì§€ì› Provider

| Provider | ëª¨ë¸ ì˜ˆì‹œ | íŠ¹ì§• |
|----------|-----------|------|
| **OpenAI** | gpt-4o, gpt-4-turbo, gpt-3.5-turbo | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë¨ |
| **Anthropic** | claude-3-opus, claude-3-sonnet, claude-3-haiku | ê¸´ ì»¨í…ìŠ¤íŠ¸, ì•ˆì „ì„± |
| **Custom** | ìì²´ ëª¨ë¸ | ìì²´ ë°±ì—”ë“œ ì„œë²„ |

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# .env.local
VITE_LLM_PROVIDER=openai
VITE_LLM_API_KEY=sk-your-api-key-here
VITE_LLM_MODEL=gpt-4o
```

### 2. Provider ì‚¬ìš©

```typescript
import { createLLMProvider } from '@/shared/api/llm';

const provider = createLLMProvider();

// ì±„íŒ…
const response = await provider.chat({
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Hello!' },
  ],
});

console.log(response.content);
```

---

## âš™ï¸ í™˜ê²½ ì„¤ì •

### í™˜ê²½ë³€ìˆ˜ ëª©ë¡

| ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `VITE_LLM_PROVIDER` | Provider íƒ€ì… (`openai`, `anthropic`, `custom`) | `openai` |
| `VITE_LLM_API_KEY` | API í‚¤ | - |
| `VITE_LLM_MODEL` | ê¸°ë³¸ ëª¨ë¸ | Providerë³„ ê¸°ë³¸ê°’ |
| `VITE_LLM_BASE_URL` | ì»¤ìŠ¤í…€ API ì—”ë“œí¬ì¸íŠ¸ | Providerë³„ ê¸°ë³¸ê°’ |

### Providerë³„ ì„¤ì •

#### OpenAI

```bash
VITE_LLM_PROVIDER=openai
VITE_LLM_API_KEY=sk-...
VITE_LLM_MODEL=gpt-4o
# ì„ íƒ: Azure OpenAI ì‚¬ìš© ì‹œ
VITE_LLM_BASE_URL=https://your-resource.openai.azure.com
```

#### Anthropic

```bash
VITE_LLM_PROVIDER=anthropic
VITE_LLM_API_KEY=sk-ant-...
VITE_LLM_MODEL=claude-3-5-sonnet-20241022
```

#### Custom (ìì²´ ì„œë²„)

```bash
VITE_LLM_PROVIDER=custom
VITE_LLM_API_KEY=your-auth-token
VITE_LLM_BASE_URL=https://your-api.com
```

---

## ğŸ“¡ API ì‚¬ìš©ë²•

### Provider ìƒì„±

```typescript
import { 
  createLLMProvider,
  OpenAIAdapter,
  AnthropicAdapter,
  CustomAdapter,
} from '@/shared/api/llm';

// ë°©ë²• 1: í™˜ê²½ë³€ìˆ˜ ìë™ ê°ì§€
const provider = createLLMProvider();

// ë°©ë²• 2: ëª…ì‹œì  ì„¤ì •
const provider = createLLMProvider({
  type: 'openai',
  apiKey: 'sk-...',
  defaultModel: 'gpt-4o',
});

// ë°©ë²• 3: ì§ì ‘ ì–´ëŒ‘í„° ìƒì„±
const openai = new OpenAIAdapter({
  apiKey: 'sk-...',
  defaultModel: 'gpt-4o',
});
```

### ë©”ì‹œì§€ í˜•ì‹

```typescript
import type { LLMMessage } from '@/shared/api/llm';

const messages: LLMMessage[] = [
  { role: 'system', content: 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.' },
  { role: 'user', content: 'ì•ˆë…•í•˜ì„¸ìš”!' },
  { role: 'assistant', content: 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?' },
  { role: 'user', content: 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œìš”?' },
];
```

### ì¼ë°˜ ì±„íŒ… (Non-streaming)

```typescript
const response = await provider.chat({
  messages,
  model: 'gpt-4o', // ì„ íƒ
  temperature: 0.7, // ì„ íƒ (0-2)
  maxTokens: 1024, // ì„ íƒ
});

console.log(response.content);
console.log(response.usage); // { promptTokens, completionTokens, totalTokens }
```

### ìš”ì²­ ì·¨ì†Œ

```typescript
const controller = new AbortController();

// 5ì´ˆ í›„ ì·¨ì†Œ
setTimeout(() => controller.abort(), 5000);

try {
  const response = await provider.chat({
    messages,
    signal: controller.signal,
  });
} catch (error) {
  if (error.name === 'AbortError') {
    console.log('ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.');
  }
}
```

---

## ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë°

### ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°

```typescript
const stream = provider.stream({
  messages: [{ role: 'user', content: 'Tell me a story' }],
});

let fullContent = '';

for await (const chunk of stream) {
  fullContent += chunk.content;
  console.log(chunk.content); // ì‹¤ì‹œê°„ ì¶œë ¥
  
  if (chunk.done) {
    console.log('ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ');
  }
}
```

### Reactì—ì„œ ìŠ¤íŠ¸ë¦¬ë°

```typescript
function useLLMStream() {
  const [content, setContent] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const provider = createLLMProvider();
  
  const sendMessage = async (userMessage: string) => {
    setIsLoading(true);
    setContent('');
    
    try {
      const stream = provider.stream({
        messages: [{ role: 'user', content: userMessage }],
      });
      
      for await (const chunk of stream) {
        setContent(prev => prev + chunk.content);
      }
    } finally {
      setIsLoading(false);
    }
  };
  
  return { content, isLoading, sendMessage };
}
```

### SSE í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì‚¬ìš©

```typescript
import { createSSEStream, createRetryableSSEStream } from '@/shared/api/llm';

// ê¸°ë³¸ ìŠ¤íŠ¸ë¦¼
const stream = createSSEStream({
  url: '/api/chat/stream',
  method: 'POST',
  headers: { Authorization: 'Bearer token' },
  body: { message: 'Hello' },
});

for await (const data of stream) {
  console.log(data);
}

// ì¬ì‹œë„ ë¡œì§ í¬í•¨
const reliableStream = createRetryableSSEStream(
  {
    url: '/api/chat/stream',
    method: 'POST',
    body: { message: 'Hello' },
  },
  {
    maxRetries: 3,
    initialDelay: 1000,
    maxDelay: 10000,
  }
);
```

---

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬

### LLMError íƒ€ì…

```typescript
import { LLMError } from '@/shared/api/llm';

try {
  await provider.chat({ messages });
} catch (error) {
  if (error instanceof LLMError) {
    switch (error.type) {
      case 'auth_error':
        console.log('API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”');
        break;
      case 'rate_limit':
        console.log('ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”');
        break;
      case 'invalid_request':
        console.log('ìš”ì²­ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤');
        break;
      case 'network_error':
        console.log('ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”');
        break;
      case 'timeout':
        console.log('ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤');
        break;
      default:
        console.log('ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜:', error.message);
    }
    
    console.log('ìƒíƒœ ì½”ë“œ:', error.statusCode);
  }
}
```

### Error Boundary ì‚¬ìš©

```tsx
import { ErrorBoundary, LLMErrorFallback } from '@/shared/ui/error';

function ChatPage() {
  return (
    <ErrorBoundary
      fallback={(props) => <LLMErrorFallback {...props} />}
      onError={(error, info) => {
        // ì—ëŸ¬ ë¡œê¹… ì„œë¹„ìŠ¤ì— ì „ì†¡
        errorTracker.capture(error, { extra: info });
      }}
    >
      <ChatRoom />
    </ErrorBoundary>
  );
}
```

---

## ğŸ”§ ì»¤ìŠ¤í…€ Provider ì¶”ê°€

### ìƒˆ ì–´ëŒ‘í„° êµ¬í˜„

```typescript
// src/shared/api/llm/adapters/MyCustomAdapter.ts

import type { LLMAdapter, LLMProviderConfig, LLMRequest, LLMResponse, LLMStreamChunk } from '../types';

export class MyCustomAdapter implements LLMAdapter {
  readonly name = 'my-custom';
  private config: LLMProviderConfig;
  
  constructor(config: LLMProviderConfig) {
    this.config = config;
  }
  
  async chat(request: LLMRequest): Promise<LLMResponse> {
    const response = await fetch(`${this.config.baseUrl}/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.config.apiKey}`,
      },
      body: JSON.stringify({
        messages: request.messages,
        model: request.model || this.config.defaultModel,
      }),
      signal: request.signal,
    });
    
    const data = await response.json();
    
    return {
      content: data.result,
      model: data.model,
      finishReason: 'stop',
    };
  }
  
  async *stream(request: LLMRequest): AsyncGenerator<LLMStreamChunk> {
    // ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
    // ...
    yield { content: '', done: true };
  }
}
```

### Provider íŒ©í† ë¦¬ì— ë“±ë¡

```typescript
// src/shared/api/llm/index.ts ìˆ˜ì •

import { MyCustomAdapter } from './adapters/MyCustomAdapter';

export function createLLMProvider(options?: CreateLLMProviderOptions): LLMAdapter {
  switch (type) {
    // ... ê¸°ì¡´ ì¼€ì´ìŠ¤
    case 'my-custom':
      return new MyCustomAdapter(config);
    // ...
  }
}
```

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs/api-reference)
- [Anthropic API ë¬¸ì„œ](https://docs.anthropic.com/claude/reference)
- [Architecture Guide](./ARCHITECTURE.md)

---

## â“ FAQ

### Q: API í‚¤ëŠ” ì–´ë””ì„œ ë°œê¸‰ë°›ë‚˜ìš”?

- **OpenAI**: https://platform.openai.com/api-keys
- **Anthropic**: https://console.anthropic.com/

### Q: í”„ë¡ì‹œ ì„œë²„ë¥¼ í†µí•´ ìš”ì²­í•˜ë ¤ë©´?

`.env` íŒŒì¼ì—ì„œ `VITE_LLM_BASE_URL`ì„ í”„ë¡ì‹œ ì„œë²„ ì£¼ì†Œë¡œ ì„¤ì •í•˜ì„¸ìš”:

```bash
VITE_LLM_BASE_URL=https://your-proxy-server.com/api
```

### Q: ì—¬ëŸ¬ Providerë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?

ë„¤, ê°ê° ë³„ë„ì˜ ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ë©´ ë©ë‹ˆë‹¤:

```typescript
const openai = new OpenAIAdapter({ apiKey: '...' });
const anthropic = new AnthropicAdapter({ apiKey: '...' });

// í•„ìš”ì— ë”°ë¼ ì„ íƒì  ì‚¬ìš©
const response = await (useOpenAI ? openai : anthropic).chat({ messages });
```

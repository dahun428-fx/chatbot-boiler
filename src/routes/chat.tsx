/**
 * ì±„íŒ… í˜ì´ì§€
 *
 * LLM ì±—ë´‡ UI - í™˜ê²½ë³€ìˆ˜ë¡œ LLM ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
 */
import { useRef, useEffect, useCallback } from 'react';
import { createFileRoute } from '@tanstack/react-router';
import { useRecoilState } from 'recoil';

import { ChatInput, ChatMessageList } from '@/components/chat';
import { createLLM } from '@/shared/api/llm/direct';
import type { LLMAdapter, LLMMessage, LLMProviderType } from '@/shared/api/llm/direct/types';
import { isLoadingState, messagesState, streamingMessageState } from '@/store/chat';

// í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM ì„¤ì • ë¡œë“œ
const LLM_CONFIG = {
  provider: (import.meta.env.VITE_LLM_PROVIDER || 'openai') as LLMProviderType,
  apiKey: import.meta.env.VITE_LLM_API_KEY || '',
  systemPrompt: import.meta.env.VITE_LLM_SYSTEM_PROMPT || 'You are a helpful assistant.',
};

const ChatPage = () => {
  const [messages, setMessages] = useRecoilState(messagesState);
  const [isLoading, setIsLoading] = useRecoilState(isLoadingState);
  const [streamingMessage, setStreamingMessage] = useRecoilState(streamingMessageState);

  const chatContainerRef = useRef<HTMLDivElement>(null);
  const abortControllerRef = useRef<AbortController | null>(null);
  const llmRef = useRef<LLMAdapter | null>(null);

  // ìŠ¤í¬ë¡¤ í•˜ë‹¨ ì´ë™
  const scrollToBottom = useCallback(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, []);

  useEffect(() => {
    scrollToBottom();
  }, [messages, streamingMessage, scrollToBottom]);

  // LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  const getLLM = useCallback((): LLMAdapter => {
    if (!LLM_CONFIG.apiKey) {
      throw new Error('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
    }
    llmRef.current = createLLM({ type: LLM_CONFIG.provider, apiKey: LLM_CONFIG.apiKey });
    return llmRef.current;
  }, []);

  // ë©”ì‹œì§€ ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ê³ ì •)
  const handleSend = useCallback(
    async (content: string) => {
      if (!content.trim() || isLoading) return;

      if (!LLM_CONFIG.apiKey) {
        alert('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
        return;
      }

      const userMessage: LLMMessage = { role: 'user', content };
      const newMessages = [...messages, userMessage];

      setMessages(newMessages);
      setIsLoading(true);
      setStreamingMessage('');

      try {
        const llm = getLLM();
        const requestMessages: LLMMessage[] = LLM_CONFIG.systemPrompt
          ? [{ role: 'system', content: LLM_CONFIG.systemPrompt }, ...newMessages]
          : newMessages;

        // ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
        abortControllerRef.current = new AbortController();
        let fullContent = '';

        for await (const chunk of llm.stream({
          messages: requestMessages,
          signal: abortControllerRef.current.signal,
        })) {
          fullContent += chunk.content;
          setStreamingMessage(fullContent);
          if (chunk.done) break;
        }

        setMessages([...newMessages, { role: 'assistant', content: fullContent }]);
        setStreamingMessage('');
      } catch (error) {
        if ((error as Error).name !== 'AbortError') {
          console.error('Chat error:', error);
          alert(`ì˜¤ë¥˜ ë°œìƒ: ${(error as Error).message}`);
        }
      } finally {
        setIsLoading(false);
        abortControllerRef.current = null;
      }
    },
    [messages, isLoading, getLLM, setMessages, setIsLoading, setStreamingMessage]
  );

  // ì¤‘ë‹¨
  const handleAbort = useCallback(() => {
    abortControllerRef.current?.abort();
    setIsLoading(false);
    setStreamingMessage('');
  }, [setIsLoading, setStreamingMessage]);

  return (
    <div className="flex h-[100svh] flex-col bg-white">
      {/* í—¤ë” */}
      <header className="fixed left-0 top-0 z-10 flex h-[3.5rem] w-full items-center justify-center border-b border-gray-100 bg-white">
        <h1 className="text-lg font-semibold text-gray-900">
          AI Chatbot
        </h1>
      </header>

      {/* ì±„íŒ… ì˜ì—­ */}
      <div
        ref={chatContainerRef}
        className="mt-[3.5rem] flex flex-1 flex-col overflow-y-auto px-6 pb-6 pt-[14px]"
        style={{ overscrollBehavior: 'contain' }}
      >
        <div className="mx-auto w-full max-w-4xl">
          {messages.length === 0 && !streamingMessage ? (
            <div className="flex h-full items-center justify-center p-8 text-center text-gray-500">
              <div>
                <p className="text-lg">ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”!</p>
                <p className="mt-2">ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.</p>
              </div>
            </div>
          ) : (
            <ChatMessageList
              messages={messages}
              streamingMessage={streamingMessage}
              isLoading={isLoading}
            />
          )}
        </div>
      </div>

      {/* ì…ë ¥ ì˜ì—­ */}
      <div className="mx-auto w-full max-w-4xl">
        <ChatInput
          onSend={handleSend}
          onStop={handleAbort}
          isLoading={isLoading}
        />
      </div>
    </div>
  );
};

export const Route = createFileRoute('/chat')({
  component: ChatPage,
});
